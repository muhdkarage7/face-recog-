{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830f576e-0a0a-4499-98ca-80436da36c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "# Load the InceptionV3 model with pre-trained weights\n",
    "model = InceptionV3(weights='imagenet', include_top=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453950bc-7aeb-4344-a214-b021f3e1ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "# Load the base Inception model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(229, 229, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new layers on top of the base model\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "# Add Global Average Pooling instead of Flatten\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Adding Dense layers\n",
    "model.add(layers.Dense(512, activation='relu'))  # Adding a dense layer\n",
    "model.add(layers.Dropout(0.5))  # Dropout for regularization\n",
    "model.add(layers.Dense(256, activation='relu'))  # Another dense layer\n",
    "model.add(layers.Dense(7, activation='softmax'))  # Adjust num_classes to your use case\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assign the model to Dmodel\n",
    "Dmodel = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79dbd60-609b-4d74-906d-28ed71661f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "  \n",
    "    anchor, positive, negative = y_pred\n",
    "    \n",
    "    # Ensure that the inputs are tensors\n",
    "    anchor = tf.convert_to_tensor(anchor, dtype=tf.float32)\n",
    "    positive = tf.convert_to_tensor(positive, dtype=tf.float32)\n",
    "    negative = tf.convert_to_tensor(negative, dtype=tf.float32)\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    \n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    \n",
    "    # Step 3: Subtract the two previous distances and add alpha\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    \n",
    "    # Step 4: Take the maximum of basic_loss and 0.0, then compute the mean\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e647d27-7e51-410a-ae99-7ac964c2e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def img_to_encoding(image_path, Dmodel):\n",
    "    # Load and preprocess the image\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(229, 229)) \n",
    "    img = np.around(np.array(img) / 255.0, decimals=12)  \n",
    "\n",
    "\n",
    "    x_train = np.expand_dims(img, axis=0)  \n",
    "\n",
    "    # Predict the embedding\n",
    "    embedding = Dmodel.predict_on_batch(x_train)  # Get the embedding from the model\n",
    "\n",
    "    # Squeeze to remove any single-dimensional entries\n",
    "    embedding = np.squeeze(embedding)\n",
    "\n",
    "    # Normalize the embedding to get a unit vector\n",
    "    return embedding / np.linalg.norm(embedding, ord=2)  # Normalize to unit vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463057da-04e0-41bb-870e-d81499b67eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triplets = {\n",
    "    \"nasir garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p1\\anchor.jpg\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p1\\positve.png\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p1\\negative.jpg\"\n",
    "    },\n",
    "    \"muhammad garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p2\\anchor.jpg\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p2\\positive.jpg\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p2\\negative.jpg\"\n",
    "    },\n",
    "     \"ahmad garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p3\\anchor.png\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p3\\positive.jpg\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p3\\negative.jpg\"\n",
    "    },\n",
    "     \"amina garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p4\\anchor.jpg\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p4\\positive.jpg\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\p4\\negative.jpg\"\n",
    "    },\n",
    "     \"hamida garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn5\\anchor.png\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn5\\positive.png\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn5\\negative.jpg\"\n",
    "    },\n",
    "     \"baby\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn6\\anchor.jpg\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn6\\positive.jpg\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn6\\negative.jpg\"\n",
    "    },\n",
    "     \"hauwa garba\": {\n",
    "        \"anchor\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn7\\anchor.png\",\n",
    "        \"positive\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn7\\positive.jpeg\",\n",
    "        \"negative\": r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\muhammad faces\\pn7\\negative.jpg\"\n",
    "    },\n",
    "}\n",
    "encodings = {}\n",
    "for person, images in triplets.items():\n",
    "    encodings[person] = {\n",
    "        \"anchor\": img_to_encoding(images['anchor'], Dmodel),\n",
    "        \"positive\": img_to_encoding(images['positive'], Dmodel),\n",
    "        \"negative\": img_to_encoding(images['negative'], Dmodel)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5a0283e-db0c-4638-88fd-0b3d308f7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not muhammad garba, please go away.\n",
      "distance:0.7233600616455078\n"
     ]
    }
   ],
   "source": [
    "def verify(image_path, person, triplets, Dmodel): \n",
    "    # Check if the person exists in the triplet\n",
    "    if person not in triplets:\n",
    "        print(f\"Person '{person}' not found in the database.\")\n",
    "        return None, False\n",
    "    \n",
    "    try:\n",
    "        # Compute the encoding for the provided image\n",
    "        encoding = img_to_encoding(image_path, Dmodel)\n",
    "        \n",
    "        # Compute distance using the anchor image of the specified person\n",
    "        person_encoding = img_to_encoding(triplets[person]['anchor'], Dmodel)\n",
    "        dist = np.linalg.norm(encoding - person_encoding)  # Calculate distance with anchor\n",
    "        \n",
    "        # Determine if the door should open based on the distance threshold\n",
    "        door_open = dist < 0.5  # Adjust this threshold as necessary\n",
    "        \n",
    "        # Provide feedback based on verification result\n",
    "        if door_open:\n",
    "            print(f\"It's {person}, welcome in!\")\n",
    "        else:\n",
    "            print(f\"It's not {person}, please go away.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None, False\n",
    "    \n",
    "    return dist, door_open\n",
    "\n",
    "# Example usage\n",
    "img_path = r\"C:\\Users\\Garba Buhari\\Desktop\\face recognition CNN\\Screenshot_20200908-222228.png\"\n",
    "person = \"muhammad garba\"\n",
    "dist, door_open = verify(img_path, person, triplets, Dmodel)\n",
    "print(f\"distance:{dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c989d24-ec95-4d66-9c5e-82906ea27a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
